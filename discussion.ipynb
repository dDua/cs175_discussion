{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1.) Challenges with text representation <br>\n",
    "2.) What is a Context? <br>\n",
    "2.) Transforming Discrete Space to Continuous Space <br>\n",
    "3.) Word2Vec model <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges with text representation\n",
    "\n",
    "1.) Given a collection of words in a sentences, how do we represent a word in our computer? <br> \n",
    "2.) Images have pixel values, a grey scale pixel value 0.67 is close to 0.68, but in images how do we know that \"appple\" is just a mis-spelt \"apple\", so should be close to each other. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting the context\n",
    "\n",
    "\"You shall know a word by the company it keeps\" - Firth 1957 <br>\n",
    "\n",
    "1.) It can be words in same sentence or paragraph or document.  <br>\n",
    "2.) Word in front and behind a particular word.  <br>\n",
    "3.) 'K' word in front and behind a particular word.  <br>\n",
    "\n",
    "Below is a representation that considers 2 words before and after as a context <br>\n",
    "<br>\n",
    "<font color=green>A bottle of __tezguino__ is on the table. <br>\n",
    "__Tezguino__ makes you drunk.<br>\n",
    "... <br>\n",
    "I had a fancy bottle of __wine__ and got drunk last night! <br>\n",
    "The terrible __wine__ is on the table. <br>\n",
    "\n",
    "</font>\n",
    "           \n",
    "|          |  bottle |  table  |   you   | terrible |\n",
    "|------- --|---------|---------|---------|----------|\n",
    "| tezguino |    1    |    0    |    1    |     0    | \n",
    "|   wine   |    1    |    0    |    0    |     1    | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Discrete Space to Continuous Space\n",
    "\n",
    "As we saw above, the idea is to exploit the neighbourhood co-ocuurences and learn <br>\n",
    "$$p(x_i|x_{i-k}, x_{i-(k-1)},..x_{i-1},x_{i+1},...x_{i+k} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cbow.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['when', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i ], test_sentence[i+1]], test_sentence[i+2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CBOWLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOWLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        out = embeds.sum(dim=0)\n",
    "        out = self.linear1(out).view(1,-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch:0 is 4.895758152008057 \n",
      "Loss at epoch:10 is 4.779870986938477 \n",
      "Loss at epoch:20 is 4.668769359588623 \n",
      "Loss at epoch:30 is 4.561984539031982 \n",
      "Loss at epoch:40 is 4.45906400680542 \n",
      "Loss at epoch:50 is 4.3595757484436035 \n",
      "Loss at epoch:60 is 4.263116359710693 \n",
      "Loss at epoch:70 is 4.169312953948975 \n",
      "Loss at epoch:80 is 4.077834129333496 \n",
      "Loss at epoch:90 is 3.988384485244751 \n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.CrossEntropyLoss() # LogSoftMax + NLLLoss(classification loss)\n",
    "model = CBOWLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "    \n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = list(map(lambda w: word_to_ix[w], context))\n",
    "        \n",
    "        context_var = Variable(torch.LongTensor(context_idxs))\n",
    "    \n",
    "        # Step 2. Recall that torch *accumulates* gradients.  Before passing in a new instance,\n",
    "        # you need to zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "    \n",
    "        # Step 3. Run the forward pass, getting probabilities over next words\n",
    "        probs = model(context_var)\n",
    "    \n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, Variable(torch.LongTensor([word_to_ix[target]])))\n",
    "    \n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data[0]\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Loss at epoch:{0} is {1} \".format(epoch, loss.data[0]))\n",
    "    losses.append(total_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic space learned by Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load a pre-trained model trained on Gigaword corpus as the power of these representations come from\n",
    "# a large amount of data.\n",
    "\n",
    "#To run this first downlaod the embeddings from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.720359742641449),\n",
       " ('pear', 0.6450697183609009),\n",
       " ('fruit', 0.6410146951675415),\n",
       " ('berry', 0.6302294731140137),\n",
       " ('pears', 0.6133959889411926),\n",
       " ('strawberry', 0.605826199054718),\n",
       " ('peach', 0.6025872230529785),\n",
       " ('potato', 0.5960936546325684),\n",
       " ('grape', 0.5935863256454468),\n",
       " ('blueberry', 0.5866668224334717)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['apple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dinners', 0.7902063727378845),\n",
       " ('brunch', 0.7900512218475342),\n",
       " ('Dinner', 0.7639395594596863),\n",
       " ('supper', 0.7596098184585571),\n",
       " ('luncheon', 0.7099569439888),\n",
       " ('banquet', 0.7032414674758911),\n",
       " ('breakfast', 0.7007029056549072),\n",
       " ('buffet_dinner', 0.6914125084877014),\n",
       " ('meal', 0.6843624114990234),\n",
       " ('lunch', 0.6815704703330994)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['dinner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192315101624),\n",
       " ('monarch', 0.6189672946929932),\n",
       " ('princess', 0.5902429819107056),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377322435379028),\n",
       " ('kings', 0.5236843824386597),\n",
       " ('Queen_Consort', 0.5235944986343384),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098592638969421),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# woman + king - man = queen\n",
    "\n",
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Approach : Skip-Gram\n",
    "\n",
    "<img src=\"skip.png\">\n",
    "\n",
    "The above model performs better but takes a long time to train due to high number of parameters. There are few tricks like negative sampling which are used to make this faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[Mikolov et.al] Distributed Representations of Words and Phrases and their Compositionality <br>\n",
    "[Mikolov et.al]  Efficient Estimation of Word Representations in Vector Space\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
